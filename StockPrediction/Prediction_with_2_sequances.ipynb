{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM prediction - many to many\n",
    "Inputs are 2 sequances (week_num and isHoliday) and output is the corresponding sequance of weekly_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lengyel/anaconda3/envs/MyEnv/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import datetime # week number computation\n",
    "from numpy import newaxis\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "print ('import completed')\n",
    "\n",
    "# Loading data\n",
    "dataset = pd.read_csv('/home/lengyel/Desktop/kaggle dataset/walmart/train.csv', parse_dates=['Date'])\n",
    "dataset = dataset[dataset.Dept==1][dataset.Store==1] #Selecting Department and Store No.1 to reduce complexity\n",
    "\n",
    "week_num = [\n",
    "    datetime.date(dataset['Date'][i].year, dataset['Date'][i].month, dataset['Date'][i].day).isocalendar()[1]\n",
    "           for i in range(len(dataset))]\n",
    "dataset['Week_num'] = week_num\n",
    "dataset = dataset.drop(columns=['Store','Dept','Date']) # These columns are unnecessary due to selecting \n",
    "                                                    # a specific Store and a Department, and adding week_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Map isHoliday True/False data to 0s and 1s\n",
    "dataset['IsHoliday'] = dataset['IsHoliday'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining splitting ratios\n",
    "splitrate_train = 0.6\n",
    "splitrate_val = 1-splitrate_train/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset to train and test datasets\n",
    "# creating 2 input sequences\n",
    "\n",
    "split_ix_train = int(len(dataset)*splitrate_train)\n",
    "split_ix_val = int((len(dataset)-split_ix_train)/2)\n",
    "\n",
    "train_in1 = array([dataset['Week_num'][i] for i in range(split_ix_train)]).astype(np.float64)\n",
    "train_in2 = array([dataset['IsHoliday'][i] for i in range(split_ix_train)])\n",
    "train_out = array([dataset['Weekly_Sales'][i] for i in range(split_ix_train)]).astype(np.float64)\n",
    "\n",
    "test_in1 = array([dataset['Week_num'][i] for i in range(split_ix_train,len(dataset))]).astype(np.float64)\n",
    "test_in2 = array([dataset['IsHoliday'][i] for i in range(split_ix_train,len(dataset))])\n",
    "test_out = array([dataset['Weekly_Sales'][i] for i in range(split_ix_train,len(dataset))]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape datasets to fit LSTM input\n",
    "train_in1 = train_in1.reshape(train_in1.shape[0], 1, 1)\n",
    "train_in2 = train_in2.reshape(train_in2.shape[0], 1, 1)\n",
    "train_out = train_out.reshape(train_out.shape[0], 1, 1)\n",
    "\n",
    "test_in1 = test_in1.reshape(test_in1.shape[0], 1, 1)\n",
    "test_in2 = test_in2.reshape(test_in2.shape[0], 1, 1)\n",
    "test_out = test_out.reshape(test_out.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating 2 input sequances  to 1, both for training and validating\n",
    "train_in = array([(train_in1[i, 0,0], train_in2[i, 0,0]) for i in range(train_in1.shape[0])])\n",
    "test_in = array([(test_in1[i, 0,0], test_in2[i, 0,0]) for i in range(test_in1.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping input and output sequances to fit LSTM 3D\n",
    "train_in = train_in.reshape(train_in.shape[0], 1, train_in.shape[1])\n",
    "test_in = test_in.reshape(test_in.shape[0], 1, test_in.shape[1])\n",
    "\n",
    "train_out = train_out.reshape(train_out.shape[0],1, 1)\n",
    "test_out = test_out.reshape(test_out.shape[0],1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting test dataset to validation and test datasets\n",
    "val_in = test_in[:,35:,:]\n",
    "test_in = test_in[:,:35,:]\n",
    "\n",
    "val_out = test_out[:,35:,:]\n",
    "test_out = test_out[:,:35,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing inputs \n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "train_in_scaled, test_in_scaled = train_in.copy(), test_in.copy()\n",
    "\n",
    "scaled_array=scaler.fit_transform(train_in[:,:,0])\n",
    "train_in_scaled[:,:,0] = scaled_array.reshape(train_in[:,:,0].shape[0], 1)\n",
    "\n",
    "scaled_array=scaler.fit_transform(test_in[:,:,0])\n",
    "test_in_scaled[:,:,0] = scaled_array.reshape(test_in[:,:,0].shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing outputs\n",
    "\n",
    "train_out_scaled = scaler.fit_transform(train_out[:,:,0])  \n",
    "train_out_scaled = train_out_scaled.reshape(train_out_scaled.shape[0],1, 1)\n",
    "\n",
    "test_out_scaled = scaler.fit_transform(test_out[:,:,0])  \n",
    "test_out_scaled = test_out_scaled.reshape(test_out_scaled.shape[0],1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp\n",
    "val_in_scaled = test_in_scaled[:split_ix_val]\n",
    "val_out_scaled = test_out_scaled[:split_ix_val]\n",
    "\n",
    "test_in_scaled = test_in_scaled[split_ix_val:]\n",
    "test_out_scaled = test_out_scaled[split_ix_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(7, activation='relu', return_sequences=True, batch_input_shape=(None,1, 2)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(train_in_scaled, train_out_scaled, epochs=1100, verbose=0,\n",
    "                    validation_data = (val_in_scaled, val_out_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.plot(epoch_count, training_loss, 'ro', c='r')\n",
    "plt.plot(epoch_count, test_loss, 'ro',c='b')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim(top=1, bottom=0)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(test_in_scaled, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,5))\n",
    "plt.plot(test_out_scaled.reshape(test_out_scaled.shape[0]), 'ro', c='b')\n",
    "plt.plot(yhat.reshape(yhat.shape[0]), 'ro', c='r')\n",
    "\n",
    "plt.legend(['Actual value', 'Prediction'])\n",
    "#plt.ylim(top=1, bottom=-0.3)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
